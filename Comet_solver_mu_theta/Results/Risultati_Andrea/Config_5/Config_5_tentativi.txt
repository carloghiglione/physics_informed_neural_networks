#####################################
#TENTATIVO 01

n_mu_train    = 6
n_theta_train = 10

rete:       dense(20) -> dense(20) -> dense(20)
activation: tanh


loss:
normalization: si
weigths: fit 1 | PDE 1 | BC 1

training:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses: loss = 7.085e-04 | fit: 1.891e-02^2  PDE: 1.691e-02^2  BC: 8.069e-03^2   				|| fit: 3.735e-02^2  PDE: 2.421e-01^2  BC: 6.630e-02^2  


commenti: 

Dato che ci siamo resi conto che ci potrebbero essere dei problemi, abbiamo ripreso dal codice senza sottogriglia, ma con tabella più difficile di mu e theta. Indubbiamente c'è un peggioramento nelle performances di train, inoltre c'è un forte peggioramento nelle performances di test. Adesso provo ad aumentare la dimensione della rete per vedere se riusciamo a stabilizzare il risultato del test



#####################################
#TENTATIVO 02

n_mu_train    = 6
n_theta_train = 10

rete:       dense(22) -> dense(22) -> dense(22)
activation: tanh


loss 1:
normalization: si
weigths: fit 1 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses: loss = 7.655e-04 | fit: 1.827e-02^2  PDE: 1.780e-02^2  BC: 1.071e-02^2   				|| fit: 2.941e-02^2  PDE: 1.087e-01^2  BC: 4.725e-02^2  

commenti: 

C'è stato un miglioramento dei valori e del chart, ma non sembra che il plot sia minimamente cambiato. Il prossimo passo è dare ancora più peso al fit.



#####################################
#TENTATIVO 03

n_mu_train    = 6
n_theta_train = 10

rete:       dense(22) -> dense(22) -> dense(22)
activation: tanh


loss 1:
normalization: si
weigths: fit 10 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses: loss = 1.264e-03 | fit: 7.213e-03^2  PDE: 2.263e-02^2  BC: 1.523e-02^2   
			|| fit: 2.478e-02^2  PDE: 8.237e-01^2  BC: 8.486e-02^2  

commenti: 
C'è un netto miglioramento della fit loss, sia in training che in test. Purtroppo le altre loss peggiorano nettamente, quindi ora proviamo a migliorare le performances del modello aumentando ancora i neuroni



#####################################
#TENTATIVO 04

n_mu_train    = 6
n_theta_train = 10

rete:       dense(24) -> dense(24) -> dense(24)
activation: tanh


loss 1:
normalization: si
weigths: fit 10 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses: loss = 8.448e-04 | fit: 6.347e-03^2  PDE: 1.878e-02^2  BC: 9.438e-03^2   
			|| fit: 1.789e-02^2  PDE: 2.824e-01^2  BC: 3.661e-02^2  


commenti: 

C'è un miglioramento di tutte le quantità di train. Quindi aumentare i neuroni può essere la strada. Adesso provo ancora ad aumentare i neuroni, e vedo se il tutto migliora.



#####################################
#TENTATIVO 05

n_mu_train    = 6
n_theta_train = 10

rete:       dense(26) -> dense(26) -> dense(26)
activation: tanh


loss 1:
normalization: si
weigths: fit 10 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses:8.617e-04 | fit: 6.060e-03^2  PDE: 1.941e-02^2  BC: 1.084e-02^2   
		|| fit: 1.662e-02^2  PDE: 1.159e+00^2  BC: 2.498e-02^2  


commenti: 
C'è un peggioramento complessivo della qualità della soluzione, quindi aumentare la rete era grande il giusto. Ora torno alla configurazione con 24 neuroni, ma amplio il dataset



#####################################
#TENTATIVO 06

n_mu_train    = 10
n_theta_train = 10

rete:       dense(24) -> dense(24) -> dense(24)
activation: tanh


loss 1:
normalization: si
weigths: fit 10 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 9000

Losses: loss = 8.307e-04 | fit: 6.198e-03^2  PDE: 1.873e-02^2  BC: 9.781e-03^2   
			|| fit: 1.559e-02^2  PDE: 4.051e-02^2  BC: 1.532e-02^2  


commenti: 

Ho usato il dataset ampliato di tables_2, con molti più mus (13223.404098 s). Finalmente è riuscito ad apprendere anche PDE nei parametri di test. Quindi adesso sembra che abbiamo imparato la soluzione della comet equations anche per valori più bassi di mu



#####################################
#TENTATIVO 07

n_mu_train    = 10
n_theta_train = 10

rete:       dense(25) -> dense(25) -> dense(25)
activation: tanh


loss 1:
normalization: si
weigths: fit 10 | PDE 1 | BC 1

training 1:
adam(lr 1e-3): n_epoch= 1000
BFGS:          n_epoch= 10000

Losses: loss = 8.692e-04 | fit: 6.585e-03^2  PDE: 1.750e-02^2  BC: 1.137e-02^2   
			|| fit: 3.630e-02^2  PDE: 6.765e-02^2  BC: 4.840e-02^2  


commenti: 
Ho usato ancora il dataset ampliato di tables_2, con molti più mus. C'è palesemente molto overfitting, quindi il modello migliore attualmente è 06. Inoltre ha impiegato ben 15729.81 s
